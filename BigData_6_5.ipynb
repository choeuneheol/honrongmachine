{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData 6.5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3UkjN7CPzI/PZNf/g4Ya8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choeuneheol/honrongmachine/blob/main/BigData_6_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12th Phase\n",
        "\n",
        "Similarity, Neighbors,and Clusters\n",
        "\n",
        "유사도\n",
        "\n",
        "  비즈니스 문제에서 유사도는 데이터분석 기법과 함께\n",
        "\n",
        "중요한 해결책의 기반이 됨\n",
        "\n",
        "\n",
        "  두 객체가 어떤 측면에서 비슷하다면 다른 부분에서도 유사성을보임\n",
        "\n",
        "\n",
        "비즈니스 사례\n",
        "\n",
        "IBM\n",
        "\n",
        "  영업부서에서 잠재고객을 찾기 위해 우수 고객과 유사한 회사 찾음\n",
        "\n",
        "HP \n",
        "\n",
        "  많은 고객 사의 서버를 유지보수\n",
        "\n",
        "  어느 서버 환경 설정이 가장 좋은 성능을 내는지 찾음.\n",
        "\n",
        "아마존 or 넷플릭스\n",
        "\n",
        "  유사도 축정(X를 구매한 고객이 Y도 구매했다.)\n",
        "\n",
        "\n",
        "유사도와 거리\n",
        "\n",
        "유사도의 정의\n",
        "\n",
        "  데이터 가나의 유사도 측정 필요\n",
        "\n",
        "  특징을 정의하는 공간에서 가까울수록 두 객체는 비슷\n",
        "\n",
        "  유사하다는 근거 -> 거리\n",
        "\n",
        "거리\n",
        "\n",
        "  데이터와 데이터 간의 거리는 어떻게 측정?\n",
        "\n",
        "유사도와 거리\n",
        "\n",
        "객체간의 거리와 유사도\n",
        "\n",
        "###(유클리드 거리에서 임의로 식을 늘린다.)\n",
        "\n",
        "최근접 이웃 추론(Nearest-Neighbor Reasoning)\n",
        "\n",
        "확률 추정\n",
        "\n",
        "  Supervised Learning\n",
        "\n",
        "  K 개의 이웃 중 Voting\n",
        "\n",
        "회귀분석 적용 가능\n",
        "\n",
        "  David 의 소득예측\n",
        "  \n",
        "  가까운 JOhn, Rachael, Norah 의 소득으로 David 소득예측\n",
        "  평균값 42\n",
        "  중간값 40\n",
        "\n",
        "확률계산\n",
        "\n",
        "  모든 이웃은 똑같이 다루어야 할까?\n",
        "\n",
        "  해결방법?\n",
        "\n",
        "이웃의 수\n",
        "\n",
        "  이웃의 수는 최종적으로 투표방법을 많이 사용하므로 홀수 사용하므로\n",
        "  데이터 수가 작을 경우 \n",
        "  ###(홀수 사용)\n",
        "\n",
        "  K-NN (K-Nearest Neighbor)\n",
        "\n",
        "  K 가 작아질 경우 => Overfitting\n",
        "\n",
        "  K 가 커질 경우 => Underfitting\n",
        "  ###(오버피팅, 언더피팅 일경우 둘다 데이터가 좋지 않음) \n",
        "\n",
        "\n",
        "확률계산 시 모든 이웃을 똑같이 다루어야 하나?\n",
        "  \n",
        "  확률계산 시 거리 차의 반영은 어떻게?\n",
        "\n",
        "  먼 거리의 객체가 같은 확률 값을 갖는 것이 옳은가?\n",
        "\n",
        "  가중치 적용 확률계산\n",
        "\n",
        "  ###(거리를 확률로 바꿀 수 있는가? 비율 = 확률)\n",
        "  ###(거리에 가까울수록 확률을 올려주고 거리가)\n",
        "  ###(멀수록 확률을 내려주는 알고리즘)\n",
        "  ###(제곱의역수 - 거리를 확률로 바꾸는 방법?)\n",
        "  ###(similarty wheight다더한거분에 레이첼?)\n",
        "  \n",
        "시각화, 과적합, 복잡도 제외\n",
        "  최근접 이웃 기법으로 분류 영역 시각화\n",
        "  ###(그래프에서 혼자 다른곳에 있는걸 아일랜드라고한다)\n",
        "  ###(그래프에서 아일랜드가 떠있다면 오버피팅 되어있다고 생각해야한다)\n",
        "\n",
        "  K 가 1일때와 30일 때의 시각화 결과\n",
        "\n",
        "  ###(train으로 아무것도 안해서 게으른 알고리즘)\n",
        "  \n",
        "  ###(K값이 커지면 언더피팅)\n",
        "  \n",
        "  ###(K값이 작으면 오버피팅)\n",
        "  \n",
        "  ###(K값을 보면서 오버와 언더가 일일이보면서 정해야한다.)\n",
        "  \n",
        "  ###(일일이 봐야하기 때문에 손이 많이 간다)\n",
        "\n",
        "문제점\n",
        "  \n",
        "  모델 명료성\n",
        "  \n",
        "  다른 이에게 이해시키기 어려운 문제점.\n",
        "\n",
        "  결과에 대한 명료성과 정당성이 떨어짐\n",
        "\n",
        "  - 데이터에서 어떤 ' 지식 ' 을 알아냈는지 어려움\n",
        "\n",
        "  - 수학적인 객관성이 떨어짐\n",
        "\n",
        "  - 모델 명료성과 정당성이 중요한 경우 사용불가\n",
        "\n",
        "  ###(성능은 나오는데 설명이 힘듬)\n",
        "  \n",
        "  ###(수치화, 정규화)\n",
        "\n",
        "  차원\n",
        "  \n",
        "  수치형 속성일 경우 범위가 너무 넓은 경우\n",
        "\n",
        "  속성이 너무 많은 경우\n",
        "\n",
        "  특징선택(Feature Selection) 기법\n",
        "  \n",
        "  ###(쓸때없는 것은 지우는것이 났다.)\n",
        "\n",
        "  도메인지식\n",
        "\n",
        "  - 이미 알고 있는 지식이 있다면 이를 이용해 가중치 조절.\n",
        "\n",
        "  - 만약 특정 속성이 큰 영향이 없다는 지식이 있다면 이를 아예 연산에\n",
        "\n",
        "    참여하지 못하도록 제거하는 것도 가능\n",
        "\n",
        "  계산 효율성\n",
        "    \n",
        "    실시간으로 해결해야 하는 문제에서는 적용이 힘들 수 있음\n",
        "\n",
        "  ###(계산양이 많다)\n",
        "\n",
        "  ###(성능이 아주 안나오는것도 아니다.)\n",
        "\n",
        "이질적인 속성사례\n",
        "\n",
        " 예제\n",
        "\n",
        " 성별 정보\n",
        "\n",
        " 소득의 범위 \n",
        "\n",
        " 군집화\n",
        "\n",
        " 개념\n",
        "\n",
        " - 데이터에서 자연스럽게 분류되는 그룹생성\n",
        "\n",
        " - 자율 세분화 or 군집화\n",
        " \n",
        " - 같은 그룹에 들어있는 데이터 간에는 비슷하지만 다른 그룹에 있는\n",
        "\n",
        "   데이터 들과는 비슷하지 않는 그룹을 만들어내는 것\n",
        "\n",
        "계층적 군집화\n",
        "\n",
        "유사도에 따라 점들을 묶음\n",
        "\n",
        "###(가까운것과 묶고 가까운것을 묶는걸 엔드로그램이라고 한다)\n",
        "\n",
        "위스키 분석 예1\n",
        "\n",
        "  원하는 위스키를 찾고 싶다면?\n",
        "\n",
        "  위스키 맛에 대한 정의필요\n",
        "\n",
        "   형식에 맞추어 데이터 가공필요\n",
        "\n",
        "   - color : 빛깔 14 가지\n",
        "\n",
        "   - Nose : 향 12 가지\n",
        "\n",
        "   - Body : 농도 8 가지\n",
        "\n",
        "   - Palate : 맛 15 가지\n",
        "\n",
        "   - Finish : 뒷맛 19 가지\n",
        "\n",
        "위스키 사례\n",
        "\n",
        "  맛이 비슷한 위스키를 계층적으로 찾아냄\n",
        "\n",
        "  비즈니스 적용?\n",
        "\n",
        "비계층적 군집화\n",
        "\n",
        "  객체 그룹인 군집 자체에 집중하는 방법\n",
        "\n",
        "  - 각 군집을 군집의 중앙으로 표현\n",
        "\n",
        "K 평균 군집화 알고리즘\n",
        "\n",
        " 중점 기반 군집화 알고리즘 중 가장 많이 사용\n",
        "\n",
        " 알고리즘\n",
        "\n",
        " - 먼저 원하는 군집의 개수를 성정 ( k 값 선정)\n",
        "\n",
        " - 선정된 중점을 기반으로 가장 가까운 군비들을 생성\n",
        "\n",
        " - 생성된 군집에서 다시 중점을 계산\n",
        "\n",
        "K 평균 군집화 알고리즘\n",
        "\n",
        " ###(모든데이터가 입력된 데이터 거리를 잼)\n",
        " \n",
        " ###(그리고 가까운 입력된 데이터로 감)\n",
        "\n",
        " ###(x,y를 구하라는 것은 평균값을 구하라는 것은)\n",
        " \n",
        " ###(군집의 중앙으로 간다)(계속 반복한다)\n",
        "\n",
        " ###(Distotio E(거리)제곱) <- 왜곡값\n",
        "\n",
        " ###(왜곡값이 완만해지는 부분) <- 엘보메서드\n",
        "\n",
        "비즈니스 뉴스 기사 군집화\n",
        " \n",
        " 데이터\n",
        " \n",
        " - 톰슨 로이더 텍스트 리서치 컬렉션\n",
        "\n",
        " - TRC3 는 로이더 통신사 뉴스 집대성, 오픈 되어 있음.\n",
        "\n",
        " - 2008 년 1월 ~ 2009년 2월 까지 1,800,370 의 기사로 구성\n",
        "\n",
        " 데이터 준비\n",
        " \n",
        " - Apple 관련 기사 추출\n",
        "\n",
        " - 총312개로 기사 추출\n",
        "\n",
        " - 모두 대문자 변환 후 한 문서에서만 나오는 희귀한 문자나 너무 자주 \n",
        "\n",
        "   나오는 문자는 코퍼스에서 제외\n",
        "\n",
        " - TFIDF 값을 이용해 문서에서 나온 단어에 점수부여 후 횟수에 따라 점수부여\n",
        "   \n",
        "   ###(몽땅 대문자로 하든 소문자로 바꾼다)\n",
        "   \n",
        "   ###(빈발도에 따라 주제로 인식)\n",
        "\n",
        "   ###(인포메이션게인과 비슷하다)\n",
        "\n",
        "   ###(역문서 빈발도? 말뭉치?)\n",
        "\n",
        "비즈니스 뉴스 기사 군집화\n",
        "\n",
        " 뉴스 기사 군집\n",
        "  \n",
        "  9개의 그룹으로 군집화\n",
        "\n",
        "  -1 군집 주가등급이나 목표가격 조정관련 증권분석가 관련기사\n",
        "\n",
        "  -2 군집 애플의 매일매일 주가변동에 관련된 기사\n",
        "\n",
        "  -3 군집 스티브 잡스의 건강관련 기사\n",
        "\n",
        "  -4 군집 애플이 고시한 내용의 기사\n",
        "\n",
        "  -5 군집 아이폰 관련 기사\n",
        "\n",
        "  -6 군집 장내, 장외 주가변동 기사\n",
        "\n",
        "  -7 군집 일관된 주제 없음\n",
        "\n",
        "  -8 군집 아이튠즈 관련 기사\n",
        "\n",
        "  -9 군집 로이터 특유의 뉴스요약 기사\n",
        "\n",
        "  ###(모여있는 군집을 열어서 사후 분석을 해야한다)\n"
      ],
      "metadata": {
        "id": "gN7GaZQE9Jxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14th Bayes Rule \n",
        "\n",
        " ###(이벤트 다 끝났는데 역으로 물어보는 언어)\n",
        " ###(ELORating 기법) 단일랭킹방식\n",
        "\n",
        "사후 확률\n",
        "  \n",
        "  이벤트가 모두 끝난 후 역으로 물어보는 확률\n",
        "\n",
        "  사후 추정은 쉽지 않음\n",
        "\n",
        "  주사위\n",
        "\n",
        "  - 주사위 던졌을 때 3이 나올 확률\n",
        "\n",
        "  - x는 랜덤 변수\n",
        "\n",
        "  확률분포함수\n",
        "\n",
        "  확률질량함수(pmf) -> probability mass function\n",
        "\n",
        "  확률 밀도 함수(pdf) -> probability distribution function\n",
        "\n",
        "  ###(그래프 주석에 대문자는 - 셀수 있는 데이터)\n",
        "\n",
        "  ###(그래프 주석에 소문자는 - 촘촘하게 연속데이터)\n",
        "\n",
        "확률 실험\n",
        "\n",
        "  주머니에서 상자를 결정하고 상자에서 공의 색상을 결정하는 실험\n",
        "\n",
        "  랜덤변수 X={A,B}, Y={파란공, 흰공}\n",
        "\n",
        "확률\n",
        "\n",
        "  상자는 A가 선택될 확률은?\n",
        "  \n",
        "  - 사전확률 P(X=A)=P(A)=7/10\n",
        "  \n",
        "  ###(앞단을 물어볼 확률)\n",
        "\n",
        "\n",
        "  상자 A가 선택된 상태에서 공은 흰공이 뽑힐 확률은?\n",
        "  \n",
        "  - 조건부 확률 P(Y=흰공|X=A)=P(흰공|A)=2/10\n",
        "  \n",
        "  ###(A라는 상자에서 흰공이 뽑힐 확률)\n",
        "\n",
        "\n",
        "  상자는 A가 선택되고 공은 흰 공이 뽑힐 확률은?\n",
        "\n",
        "  - 결합 확률 P(A,흰공)=P(흰공|A)P(A)=(2/10)(7/10)=7/50\n",
        "\n",
        "  ###(이벤트 앞단, 뒷단)\n",
        "  \n",
        "  ###(상자는 A가 선택되고 - 사전확률)\n",
        "  \n",
        "  ###(공은 흰 공이 뽑힐 확률은? - 조건부확률)\n",
        "\n",
        "  흰 공이 나올 확률은?\n",
        "  \n",
        "  - 주변 확률 P(흰공)=P(흰공|A)P(A)+P(흰공|B)P(B)\n",
        "  \n",
        "                     =(2/10)(7/10)+(9/15)(3/10)=8/25\n",
        "\n",
        "  - P(X,Y)=P(X)P(Y)이면 X와 Y는 독립\n",
        "\n",
        "  ###(사전확률과 주변확률 앞단과 뒷단 차이가 있다)\n",
        "\n",
        "이런 문제를 생각해 보자\n",
        "  \n",
        "  - 흰 공이 뽑혔는데 어느 상자에 나왔는지 맞추어라\n",
        "   \n",
        "   기본전략\n",
        "\n",
        "    - 흰공이 상자 A와 B에서 나왔을 확률을 각각 구하여 큰 값을선택\n",
        "  \n",
        "  계산은 어떻게?\n",
        "\n",
        "  ###(흰공을 뽑았는데 A에서 나왔니 B에서 나왔니? - 사후확률 )\n",
        "  ###(조건부 확률이 큰쪽으로 고르란이야기?)\n",
        "\n",
        "\n",
        "첫번째 제안\n",
        "  상자 A에서 흰공이 나올 확률과 상자 B에서 흰공이 나올 확률을 교하여 큰쪽을 취함\n",
        "\n",
        "  P(흰공|B)=9/15 > P(흰공|A)=2/10\n",
        "\n",
        "  -> 흰공은 상자 B에서 나왔다고 판정\n",
        "\n",
        "  조건부 확률 P(Y|X)사용 -> 맞는 접근인지?\n",
        "\n",
        "  조건부 확률을 우도라고 부르기도 함\n",
        "\n",
        "\n",
        "\n",
        "두번째 제안\n",
        "  \n",
        "  상자 A가 선택될 확률과 상자 B가 선택될 확률을 비교하여 큰쪽을 취함\n",
        "\n",
        "   - P(A)=7/10 > P(B)=3/10 이므로 '상자 A에서 나왔다'고 말함\n",
        "\n",
        "   - 사전확률 P(X) 사용 -> 맞는 접근인지?\n",
        "\n",
        "올바른 생각\n",
        "\n",
        " 두가지 제안의 한계\n",
        "\n",
        " 사후확률 추정\n",
        "  \n",
        "  -P(A|흰공)과P(B|흰공)을 비교 후 큰 쪽을 취함\n",
        "  \n",
        "  -P(X|Y) 사용\n",
        "  \n",
        "  -P(X|Y)를 사후 확률 이라 함\n",
        "  \n",
        "  계산가능한가?\n",
        "\n",
        "베이스 정리의 유도\n",
        "\n",
        " P(X,Y) = P(Y,X)\n",
        "\n",
        " P(X)P(Y|X) = P(Y)P(X|Y)\n",
        "\n",
        " P(X|Y)=P(Y)/P(Y|X)P(X)= P(Y)우도*사전확률\n",
        "\n",
        "베이스 정리를 이용한 사후 확률 계산\n",
        "\n",
        " P(A|하양)=P(하양)/P(하양|A)P(A) = (8/25)/(2/10)(7/10) = 0.4375\n",
        "\n",
        " P(B|하양)=P(하양)/P(하양|B)P(B) = (8/25)/(9/15)(3/10) = 0.5625\n",
        "\n",
        " ###(로지스틱회귀랑 비슷?)\n",
        " ###(분모 베이스정리를 날리는 식을 나이브 베이즈)\n"
      ],
      "metadata": {
        "id": "L7XRgw8i_2mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15th Model Estimate\n",
        "\n",
        "모델의 성능 평가\n",
        "\n",
        "생성해낸 분석 모델은 좋은 성능을 내고 있는가?\n",
        "\n",
        "좋은 성능은 무엇을 의미하는가?\n",
        "\n",
        "성능평가는 어떤 있는가?\n",
        "\n",
        " 범주형-\n",
        "\n",
        " 수치형-\n",
        "\n",
        "비용(RIsk)\n",
        " \n",
        " 오 분류 시 생기는 분류 비용 고려\n",
        "\n",
        " 잘 된 분류와 잘못된 분류를 좀 더 세분하게 나누어 평가\n",
        "\n",
        " 위음성(False Negative) 의 비용이 더 큼\n",
        "\n",
        "정확도\n",
        " \n",
        " 정확도는 너무 단순하여 성능평가에 어려움이 있음\n",
        "\n",
        " 정확도 = (올바르게 판단한 횟수)/(전체 결정 횟수)\n",
        "\n",
        " 정확도 = 1 - 오류율\n",
        "\n",
        "혼동행렬\n",
        "\n",
        "\n",
        "            Positive               Negative\n",
        "\n",
        "True     올바른양성(TP)         잘못된양성(FP)\n",
        " \n",
        "False    잘못된음성(FN)         올바른음성(TN)\n",
        "\n",
        "\n",
        "###( True, False <- M)\n",
        "\n",
        "###( 잘못된양성(FP) -내가만든 모델에서암 )\n",
        "\n",
        "2계층 문제 분류\n",
        "\n",
        "Positive (양성), Negattive(음성)\n",
        "\n",
        "-실제 타겟 값\n",
        "\n",
        "True, False\n",
        "\n",
        "- 모델이 예측한 값\n",
        "\n",
        "정확률(Precision)\n",
        "\n",
        "=TP/TP+FP \n",
        "\n",
        "###(True/Pos)\n",
        "\n",
        "재현률(Recall)\n",
        "\n",
        "=TR/TR+FN\n",
        "\n",
        "###(Pos/TP)\n",
        "\n",
        "F measure (F1 score)\n",
        "\n",
        "=precision+Recall / 2*Precision*Recall\n",
        "###(분류모델 평가할때)\n",
        "\n",
        "###(위의 식에서 분자는 다 암환자)\n",
        "\n",
        "###(기준은 내가 만든 모델 값)"
      ],
      "metadata": {
        "id": "dcrhravfRSRU"
      }
    }
  ]
}